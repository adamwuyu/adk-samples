diff --git a/.cursor/rules/right-way-to-test.mdc b/.cursor/rules/right-way-to-test.mdc
index 5e388f4..379f4e1 100644
--- a/.cursor/rules/right-way-to-test.mdc
+++ b/.cursor/rules/right-way-to-test.mdc
@@ -7,10 +7,16 @@ alwaysApply: false
 ⚠️注意：无论哪种测试方式，必须切换到adk虚拟环境（conda activate adk）。
 
 ## 基于LLM实时生成内容的集成测试
+
+### 一般测试
 conda activate adk && cd /Users/adam/adk/adk-samples/agents/draft_craft/draft_craft && python -m tests.run_flat_test --use-llm
-或者：
+
+### 输出到文件的测试
 conda activate adk && cd /Users/adam/adk/adk-samples/agents/draft_craft/draft_craft && python -m tests.run_flat_test --use-llm > run_output.txt 2>&1
 
+### 带过滤的测试
+export PYTHONWARNINGS='default'\npython -m draft_craft.tests.run_flat_test --use-llm 2>&1 | grep SCORING_DEBUG
+
 ## 基于dummy内容的集成测试
 conda activate adk
 cd /Users/adam/adk/adk-samples/agents/draft_craft/draft_craft && python -m tests.run_flat_test
diff --git a/agents/draft_craft/draft_craft/agent.py b/agents/draft_craft/draft_craft/agent.py
index b670e89..9a68c98 100644
--- a/agents/draft_craft/draft_craft/agent.py
+++ b/agents/draft_craft/draft_craft/agent.py
@@ -71,11 +71,10 @@ kingdora_api_key = os.getenv("KINGDORA_API_KEY")
 if kingdora_base_url and kingdora_api_key:
     try:
         model_instance = LiteLlm(
-            model="openai/gpt-4o-mini",
+            model="openai/gpt-4.1-nano",
             api_base=kingdora_base_url,
             api_key=kingdora_api_key,
-            stream=True,
-            temperature=0.2
+            stream=False,
         )
         logger.info("✅ 成功配置Gemini模型")
     except Exception as e:
@@ -88,7 +87,7 @@ if not model_instance and oneapi_base_url and oneapi_api_key:
             model="openai/gpt-4o", 
             api_base=oneapi_base_url,
             api_key=oneapi_api_key,
-            stream=True
+            stream=False
         )
         logger.info("✅ 成功配置GPT-4o模型")
     except Exception as e:
@@ -97,7 +96,7 @@ if not model_instance and oneapi_base_url and oneapi_api_key:
 # 如果无法配置任何模型，使用默认模型
 if not model_instance:
     logger.warning("⚠️ 未找到模型配置，使用默认GPT模型")
-    model_instance = "openai/gpt-4o-mini"
+    model_instance = "openai/gpt-4.1-nano"
 
 # --- 定义Root Agent ---
 # 扁平化设计：单一Agent + 完整Tool集，不使用循环或嵌套Agent
@@ -117,8 +116,8 @@ root_agent = Agent(
         FunctionTool(func=generate_draft_improvement),
         
         # V0.9: 新版LLM评分工具 - 符合ADK标准架构
-        FunctionTool(func=generate_draft_scoring),
-        FunctionTool(func=save_scoring_result),
+        # FunctionTool(func=generate_draft_scoring),
+        # FunctionTool(func=save_scoring_result), # 移除旧的保存评分工具，避免与家长版冲突
         
         # V1.0: 针对中等家长受众的评分工具
         score_for_parents_tool,
diff --git a/agents/draft_craft/draft_craft/prompt.py b/agents/draft_craft/draft_craft/prompt.py
index 889ef93..ab0db83 100644
--- a/agents/draft_craft/draft_craft/prompt.py
+++ b/agents/draft_craft/draft_craft/prompt.py
@@ -37,7 +37,7 @@ ROOT_AGENT_INSTRUCTION = """你是一个写作助手，帮助用户完成文稿
 - 文稿评分（只用家长受众评分）：
   1. 调用`score_for_parents`工具，提供文稿内容、受众画像和评分标准作为参数
   2. 基于返回的提示词，从家长视角评估文稿质量，严格按照提示词要求的格式输出
-  3. 使用`save_parents_scoring_result`工具保存你的评分结果
+  3. **完成后，下一步是使用合适的工具保存你的评分输出（包含分数、评价和关键问题）。**
   4. **每次保存评分后，必须立即调用`check_progress`工具，判断是否需要继续优化或终止流程。**
      - 只有当`check_progress`工具返回"需要继续"时，才可以进入下一轮写作或评分。
      - 如果`check_progress`工具返回"已完成"或"已达最大迭代次数"，则流程必须终止，不能再进行任何写作或评分操作。
@@ -57,7 +57,7 @@ ROOT_AGENT_INSTRUCTION = """你是一个写作助手，帮助用户完成文稿
 3. 确保你的输出符合提示词的格式要求
 4. 根据提示词的用途，使用相应的保存工具：
    - 对于创作/改进内容，使用`save_draft_result`保存文稿
-   - 对于评分内容，只使用`save_parents_scoring_result`
+   - 对于评分内容，使用相应的评分保存工具
 
 **评分格式说明**：
 当使用家长受众评分时，需要按照提示词要求输出：
diff --git a/agents/draft_craft/draft_craft/tests/run_flat_test.py b/agents/draft_craft/draft_craft/tests/run_flat_test.py
index 7db3b43..f391713 100644
--- a/agents/draft_craft/draft_craft/tests/run_flat_test.py
+++ b/agents/draft_craft/draft_craft/tests/run_flat_test.py
@@ -23,12 +23,39 @@ from pprint import pprint
 import re
 import uuid
 from datetime import datetime
+from dotenv import load_dotenv
+import logging
 
 from google.adk.runners import Runner
 from google.adk.sessions import InMemorySessionService
 from google.genai import types
 from draft_craft.tools import check_progress
 
+logger = logging.getLogger(__name__)
+
+# --- 自动加载.env环境变量 ---
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+AGENTS_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))
+DOTENV_PATH = os.path.join(AGENTS_DIR, '.env')
+
+if os.path.exists(DOTENV_PATH):
+    load_dotenv(dotenv_path=DOTENV_PATH)
+    print(f"[dotenv] 已从 {DOTENV_PATH} 加载.env 环境变量")
+else:
+    print(f"[dotenv] 未找到 {DOTENV_PATH}，尝试默认加载 .env")
+    load_dotenv()
+
+print("KINGDORA_API_KEY:", os.getenv("KINGDORA_API_KEY"))
+print("KINGDORA_BASE_URL:", os.getenv("KINGDORA_BASE_URL"))
+print("ONEAPI_API_KEY:", os.getenv("ONEAPI_API_KEY"))
+print("ONEAPI_BASE_URL:", os.getenv("ONEAPI_BASE_URL"))
+print("OPENAI_API_KEY:", os.getenv("OPENAI_API_KEY"))
+assert os.getenv("OPENAI_API_KEY"), (
+    "OPENAI_API_KEY 环境变量未设置！\n"
+    "请检查 agents/draft_craft/.env 文件是否存在且格式正确，"
+    "或在shell中 export OPENAI_API_KEY=xxx 后再运行。"
+)
+
 # 解析命令行参数
 parser = argparse.ArgumentParser(description='扁平化写作智能体draft_craft的集成测试')
 parser.add_argument('--use-llm', action='store_true', help='使用LLM实时生成内容，而不是模拟内容')
@@ -42,6 +69,10 @@ else:
     os.environ["USE_LLM_GENERATOR"] = "false"
     print("已启用模拟内容生成模式（默认）")
 
+# 在测试开始前重置LLM调试日志，确保日志文件创建
+from draft_craft.tools.logging_utils import reset_llm_log
+reset_llm_log()
+
 # 将项目根目录添加到sys.path
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
 PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", "..", ".."))
@@ -108,7 +139,7 @@ TEST_CASES = [
     },
     # 必不及格测试用例
     {
-        INITIAL_MATERIAL_KEY: "素材极其简略，仅一句话。",
+        INITIAL_MATERIAL_KEY: "我没有素材。",
         INITIAL_REQUIREMENTS_KEY: "写一篇结构复杂、内容丰富、案例详实的长文，要求极高。",
         INITIAL_SCORING_CRITERIA_KEY: "必须有5个真实案例，结构极清晰，语言极优美，创新性强。",
         "audience_profile": "专家级评审",
@@ -117,7 +148,7 @@ TEST_CASES = [
 ]
 
 # 默认使用必不及格测试用例
-TEST_INITIAL_DATA = TEST_CASES[-1]
+TEST_INITIAL_DATA = TEST_CASES[0]  # 使用家长教育主题测试用例
 
 async def async_main():
     log_path = os.path.join(os.path.dirname(__file__), "logs", "flat_test_debug.log")
diff --git a/agents/draft_craft/draft_craft/tools/llm_generator.py b/agents/draft_craft/draft_craft/tools/llm_generator.py
index 0072d92..53a2071 100644
--- a/agents/draft_craft/draft_craft/tools/llm_generator.py
+++ b/agents/draft_craft/draft_craft/tools/llm_generator.py
@@ -68,63 +68,36 @@ class LlmContentGenerator:
     
     def _setup_model(self, model_name: Optional[str], temperature: float) -> LiteLlm:
         """
-        配置LLM模型
-        
-        Args:
-            model_name: 模型名称，如果为None则使用环境变量配置
-            temperature: 生成温度
-            
-        Returns:
-            配置好的LiteLlm实例
+        配置LLM模型，优先使用gpt-4.1-nano（kingdora优先），否则gpt-4o，否则抛出异常。
         """
-        # 检查环境变量中的模型配置
-        oneapi_base_url = os.getenv("ONEAPI_BASE_URL")
-        oneapi_api_key = os.getenv("ONEAPI_API_KEY")
         kingdora_base_url = os.getenv("KINGDORA_BASE_URL")
         kingdora_api_key = os.getenv("KINGDORA_API_KEY")
-        
-        # 如果指定了模型名称，直接使用
-        if model_name:
+        oneapi_base_url = os.getenv("ONEAPI_BASE_URL")
+        oneapi_api_key = os.getenv("ONEAPI_API_KEY")
+
+        if kingdora_base_url and kingdora_api_key:
             try:
-                if oneapi_base_url and oneapi_api_key:
-                    return LiteLlm(
-                        model=model_name,
-                        api_base=oneapi_base_url,
-                        api_key=oneapi_api_key,
-                        temperature=temperature
-                    )
-                else:
-                    return model_name  # 直接返回模型名称，由ADK处理
+                return LiteLlm(
+                    model="openai/gpt-4.1-nano",
+                    api_base=kingdora_base_url,
+                    api_key=kingdora_api_key,
+                )
             except Exception as e:
-                logger.error(f"配置指定模型出错: {e}")
-        
-        # 优先尝试GPT-4o
-        if oneapi_base_url and oneapi_api_key:
+                logger.error(f"配置gpt-4.1-nano模型时出错: {e}")
+                raise
+        elif oneapi_base_url and oneapi_api_key:
             try:
                 return LiteLlm(
                     model="openai/gpt-4o",
                     api_base=oneapi_base_url,
                     api_key=oneapi_api_key,
-                    temperature=temperature
                 )
             except Exception as e:
-                logger.error(f"配置GPT-4o模型出错: {e}")
-        
-        # 如果无法使用GPT-4o，尝试Gemini
-        if kingdora_base_url and kingdora_api_key:
-            try:
-                return LiteLlm(
-                    model="openai/gemini-pro",
-                    api_base=kingdora_base_url,
-                    api_key=kingdora_api_key,
-                    temperature=temperature
-                )
-            except Exception as e:
-                logger.error(f"配置Gemini模型出错: {e}")
-        
-        # 如果都配置失败，使用默认
-        logger.warning("无法配置特定模型，使用默认GPT-4o-mini")
-        return "openai/gpt-4o-mini"
+                logger.error(f"配置gpt-4o模型时出错: {e}")
+                raise
+        else:
+            logger.error("未找到有效的LLM服务配置，无法初始化LLM实例。请设置KINGDORA或ONEAPI相关环境变量。")
+            raise RuntimeError("未找到有效的LLM服务配置，无法初始化LLM实例。请设置KINGDORA或ONEAPI相关环境变量。")
     
     async def generate_initial_draft(
         self, 
diff --git a/agents/draft_craft/draft_craft/tools/llm_tools.py b/agents/draft_craft/draft_craft/tools/llm_tools.py
index a7d1586..8e0ad16 100644
--- a/agents/draft_craft/draft_craft/tools/llm_tools.py
+++ b/agents/draft_craft/draft_craft/tools/llm_tools.py
@@ -10,7 +10,7 @@ from .state_manager import (
     INITIAL_SCORING_CRITERIA_KEY, CURRENT_DRAFT_KEY, CURRENT_SCORE_KEY,
     CURRENT_FEEDBACK_KEY, ITERATION_COUNT_KEY, SCORE_THRESHOLD_KEY, IS_COMPLETE_KEY
 )
-from .logging_utils import log_generation_event
+from .logging_utils import log_generation_event, reset_llm_log, log_llm_generation
 
 logger = logging.getLogger(__name__)
 
@@ -39,22 +39,41 @@ INITIAL_WRITING_PROMPT_TEMPLATE = """
 REVISION_PROMPT_TEMPLATE = """
 你是一位专业文案写手。请基于以下反馈，改进现有文稿：
 
-## 当前文稿
+### 当前文稿
 {current_draft}
 
-## 评分反馈
+### 评分反馈
 {feedback}
 
-## 评分标准
+### 评分标准
 {scoring_criteria}
 
-## 改进指南
-1. 认真分析评分反馈，找出需要改进的地方
-2. 保留原文稿的优点和核心内容
-3. 针对性地修改和补充内容
+### 改进指南
+0. **严格注意**：你必须输出完全重写的文章，而不是仅提供改进建议
+1. 你的任务是直接修改原文，产出一篇全新改进版本，而非对原文进行评论
+2. 保留原文稿的核心内容，但必须按反馈意见对内容进行实质性修改
+3. 针对性地对表达、结构和内容进行修改和补充
 4. 确保修改后的文稿更符合评分标准
+5. 禁止重复原文，必须有实质性改动
+
+【输出模板】
+请严格按照下面的模板输出，不要添加其他说明。任何不符合此格式的输出都是错误的：
+
+---
+## 改进后文稿
+<完整替换后的文章正文>
+
+---
+## 反馈回顾
+### 分数
+<在此填入0-100之间的整数>
+
+### 详细评价
+<在此填200-300字的改进评语>
 
-请输出完整的改进后文稿，而不只是修改建议。
+### 关键问题
+- <问题一>
+- <问题二>
 """
 
 SCORING_PROMPT_TEMPLATE = """
@@ -81,7 +100,7 @@ def generate_initial_draft(tool_context: ToolContext) -> Dict[str, Any]:
     生成初始文稿，基于会话状态中的素材、要求和评分标准。
     
     此工具直接在会话状态中读取素材、要求和评分标准，
-    生成初始文稿，并将结果保存回会话状态。
+    调用LLM生成初始文稿，并将结果保存回会话状态。
     
     Args:
         tool_context: ADK工具上下文，用于访问会话状态
@@ -91,6 +110,9 @@ def generate_initial_draft(tool_context: ToolContext) -> Dict[str, Any]:
     """
     logger.info("启动初始文稿生成工具...")
     
+    # 每次开始初稿生成时，重置LLM调试日志
+    reset_llm_log()
+    
     try:
         # 使用状态管理器
         state_manager = StateManager(tool_context)
@@ -115,38 +137,35 @@ def generate_initial_draft(tool_context: ToolContext) -> Dict[str, Any]:
         
         logger.info(f"准备文稿生成。素材长度:{len(material)}，要求长度:{len(requirements)}，评分标准长度:{len(criteria)}")
         
-        # 构建提示词
-        prompt = INITIAL_WRITING_PROMPT_TEMPLATE.format(
-            material=material,
-            requirements=requirements,
-            scoring_criteria=criteria
-        )
-        
-        # 使用备用文本，以防LLM调用失败
-        backup_draft = _get_backup_draft()
+        # 获取LLM内容生成器
+        from .llm_generator import get_content_generator
+        generator = get_content_generator(temperature=0.7)
         
         # 当前迭代计数
         iteration_count = state_manager.get(ITERATION_COUNT_KEY, 0)
         
+        # 调用LLM生成正文
         try:
-            # ===使用ADK标准方式调用LLM===
-            # 注意：不再直接创建LiteLlm实例，而是直接返回提示词，
-            # 由Agent自己的LLM机制处理生成
-            
-            # 直接将提示词作为结果返回，供Agent处理
-            return {
-                "status": "llm_prompt_ready",
-                "prompt": prompt,
-                "iteration": iteration_count
-            }
-            
+            draft = generator.generate_initial_draft_sync(material, requirements, criteria)
         except Exception as e:
-            logger.error(f"生成文稿提示词时出错: {e}", exc_info=True)
-            return {
-                "status": "error", 
-                "message": f"生成文稿提示词时出错: {str(e)}"
-            }
-            
+            logger.error(f"调用LLM生成初稿失败: {e}", exc_info=True)
+            draft = _get_backup_draft()
+        
+        # 存储并记录LLM生成内容
+        tool_context.state['LLM_LAST_PROMPT'] = None  # 可选：如需记录最后一次prompt可保留
+        log_llm_generation(iteration_count, "", draft, tool_name="generate_initial_draft")
+        
+        # 保存文稿内容到状态
+        save_result = state_manager.store_draft_efficiently(draft)
+        logger.info(f"文稿保存结果: {save_result}")
+        state_manager.set(ITERATION_COUNT_KEY, iteration_count + 1)
+        
+        return {
+            "status": "success",
+            "draft_summary": draft[:100] + "..." if len(draft) > 100 else draft,
+            "draft_length": len(draft),
+            "iteration": iteration_count + 1
+        }
     except Exception as e:
         logger.error(f"generate_initial_draft工具执行失败: {e}", exc_info=True)
         return {
@@ -182,6 +201,36 @@ def save_draft_result(content: str, tool_context: ToolContext) -> Dict[str, Any]
         # 获取当前迭代计数
         iteration_count = state_manager.get(ITERATION_COUNT_KEY, 0)
         
+        # 检查是否是改进的文稿（非初始文稿）
+        if iteration_count > 0:
+            # 获取当前保存的文稿用于比较
+            current_draft = state_manager.get(CURRENT_DRAFT_KEY, "")
+            
+            # 计算内容变化比例
+            import difflib
+            matcher = difflib.SequenceMatcher(None, current_draft, content)
+            similarity = matcher.ratio()
+            logger.info(f"文稿相似度: {similarity:.2f}")
+            
+            # 如果相似度过高，认为改进不充分
+            if similarity > 0.9 and len(current_draft) > 100:
+                logger.warning(f"文稿改进不充分，相似度为 {similarity:.2f}，超过阈值0.9")
+                return {
+                    "status": "error",
+                    "message": f"与前一版本相似度过高({similarity:.2f})，请按照模板进行实质性修改",
+                    "suggestion": "请确保严格按照输出模板生成完整改进后的文章，而不是仅添加评论或建议"
+                }
+                
+            # 检查是否包含分隔标记"---"，表示是否按模板格式输出
+            if "## 改进后文稿" not in content or "## 反馈回顾" not in content:
+                if iteration_count > 0:  # 仅针对改进后的稿件检查格式
+                    logger.warning("文稿未按照指定模板格式输出")
+                    return {
+                        "status": "error",
+                        "message": "未按照指定模板格式输出，缺少'## 改进后文稿'或'## 反馈回顾'部分",
+                        "suggestion": "请严格按照提示词中的输出模板格式生成内容"
+                    }
+        
         # 保存文稿内容
         save_result = state_manager.store_draft_efficiently(content)
         logger.info(f"文稿保存结果: {save_result}")
@@ -227,6 +276,10 @@ def save_draft_result(content: str, tool_context: ToolContext) -> Dict[str, Any]
                 "iteration": iteration_count
             })
             
+            # 记录LLM生成输出及对应提示词
+            last_prompt = tool_context.state.get('LLM_LAST_PROMPT', '')
+            log_llm_generation(iteration_count, last_prompt, content, tool_name="save_draft_result")
+            
             # 返回摘要信息
             return {
                 "status": "success",
@@ -303,12 +356,29 @@ def generate_draft_improvement(tool_context: ToolContext) -> Dict[str, Any]:
         
         logger.info(f"准备文稿改进。当前文稿长度:{len(current_draft)}，反馈长度:{len(feedback)}")
         
+        # 增加前缀强调格式要求
+        prefix = """请严格按照我提供的输出模板格式生成内容。
+你的任务是生成一篇完全改进后的新文章，而不是仅提供改进建议或评论。
+我将严格检查输出格式，任何不符合模板的输出都将被拒绝。
+"""
+
+        # 增加后缀提醒
+        suffix = """
+再次强调：你必须按照上述模板，输出一个包含"## 改进后文稿"和"## 反馈回顾"两部分的完整内容。
+不要简单地复制原文，应该根据反馈进行实质性修改。
+不要在正文中添加诸如"我已按要求修改"之类的元评论。
+"""
+        
         # 构建提示词
-        prompt = REVISION_PROMPT_TEMPLATE.format(
+        prompt = prefix + REVISION_PROMPT_TEMPLATE.format(
             current_draft=current_draft,
             feedback=feedback if feedback else "没有具体反馈，请对文稿进行一般性改进，使其更全面、更有深度。",
             scoring_criteria=criteria
-        )
+        ) + suffix
+        
+        # 存储并记录LLM提示词
+        tool_context.state['LLM_LAST_PROMPT'] = prompt
+        log_llm_generation(iteration_count, prompt, "", tool_name="generate_draft_improvement")
         
         # ===使用ADK标准方式调用LLM===
         # 注意：不再直接创建LiteLlm实例，而是直接返回提示词，
diff --git a/agents/draft_craft/draft_craft/tools/logging_utils.py b/agents/draft_craft/draft_craft/tools/logging_utils.py
index dff06d2..4dc05b9 100644
--- a/agents/draft_craft/draft_craft/tools/logging_utils.py
+++ b/agents/draft_craft/draft_craft/tools/logging_utils.py
@@ -4,10 +4,15 @@ import json
 import logging
 from datetime import datetime
 from typing import Any, Dict, Optional, Union
+import os
 
 # 配置根日志记录器
 logger = logging.getLogger(__name__)
 
+# === LLM专用日志 ===
+# 将LLM调试日志保存为Markdown格式，方便查看
+LLM_LOG_FILE = os.path.join(os.path.dirname(__file__), '../tests/logs/llm_generation_debug.md')
+
 def setup_logging(level=logging.INFO):
     """
     设置日志配置
@@ -116,4 +121,23 @@ def log_generation_event(
     # 记录结构化日志
     logger.info(f"生成事件: {json.dumps(log_data, ensure_ascii=False)}")
     
-    return log_data 
\ No newline at end of file
+    return log_data 
+
+def reset_llm_log():
+    """每次测试前调用，清空LLM日志文件"""
+    # 确保日志目录存在
+    log_dir = os.path.dirname(LLM_LOG_FILE)
+    os.makedirs(log_dir, exist_ok=True)
+    with open(LLM_LOG_FILE, "w", encoding="utf-8") as f:
+        f.write("")
+
+def log_llm_generation(iteration, prompt, output, tool_name: str = "unknown_tool"):
+    """每次LLM生成时调用，记录调用的工具名称、prompt和输出"""
+    # 确保日志目录存在
+    log_dir = os.path.dirname(LLM_LOG_FILE)
+    os.makedirs(log_dir, exist_ok=True)
+    with open(LLM_LOG_FILE, "a", encoding="utf-8") as f:
+        f.write(f"\n[LLM GENERATION] {datetime.now().isoformat()} 工具:{tool_name} 迭代:{iteration}\n")
+        f.write(f"[PROMPT]\n{prompt[:1000]}{'...' if len(prompt)>1000 else ''}\n")
+        f.write(f"[OUTPUT]\n{output[:1000]}{'...' if len(output)>1000 else ''}\n")
+        f.write("="*60 + "\n") 
\ No newline at end of file
diff --git a/agents/draft_craft/draft_craft/tools/scoring_tools.py b/agents/draft_craft/draft_craft/tools/scoring_tools.py
index 8b3f36d..c9494eb 100644
--- a/agents/draft_craft/draft_craft/tools/scoring_tools.py
+++ b/agents/draft_craft/draft_craft/tools/scoring_tools.py
@@ -7,7 +7,7 @@ from google.adk.tools.tool_context import ToolContext
 from .logging_utils import log_generation_event
 from .state_manager import (
     StateManager, CURRENT_SCORE_KEY, CURRENT_FEEDBACK_KEY, 
-    IS_COMPLETE_KEY, SCORE_THRESHOLD_KEY
+    IS_COMPLETE_KEY, SCORE_THRESHOLD_KEY, INITIAL_SCORING_CRITERIA_KEY, CURRENT_DRAFT_KEY
 )
 
 # 配置日志
@@ -18,73 +18,91 @@ logger = logging.getLogger(__name__)
 # 评分提示词模板
 PARENTS_SCORING_PROMPT_TEMPLATE = """
 你现在要扮演一位中等受教育水平、经济中等以上的家长，孩子处于小学高年级至高中一年级阶段。
-请基于下面提供的受众画像和评分标准，对文稿内容进行评估和反馈。
+请严格按照以下模板格式对下方文章进行评分和反馈：
 
-## 受众画像描述
-{audience_profile}
-
-## 评分标准
+### 评分标准
 {scoring_criteria}
 
-## 待评文稿
+### 待评文章
 {draft_content}
 
-请按照以下格式进行评估：
+### 输出模板  
+```
+## 分数
+<在此填入0-100之间的整数>
+
+## 详细评价
+<在此填200-300字的详细评价和可操作建议>
 
-1. 给出一个0-100分的总体评分（仅一个整数）。
-2. 提供200-300字的详细评价和具体建议，着重指出文稿的优点和需要改进的地方。
-3. 列出2-3个关键问题或改进点（如果有的话），每条一行，以"-"开头。
+## 关键问题
++- <问题一>
++- <问题二>
+```
 
-你的评分和反馈必须从目标家长的视角出发，考虑他们的关注点、语言偏好和价值观。
+请仅输出上述模板内容，不要额外添加其他说明或格式。
 """
 
 def score_for_parents(
-    draft_content: str,
-    audience_profile: str,
-    scoring_criteria: str,
-    tool_context: Optional[ToolContext] = None
+    tool_context: ToolContext,
+    audience_profile: Optional[str] = None,
+    scoring_criteria: Optional[str] = None
 ) -> Dict:
     """
     针对中等受教育水平、经济中等以上的家长（孩子小学高年级至高一），
-    结合受众画像和评分标准，对draft_content进行多维度评分和详细反馈。
-    返回结构化分数、反馈和关键问题。
+    结合受众画像和评分标准，对当前会话状态中的文稿进行多维度评分和详细反馈。
+    返回用于LLM评分的提示词。
     输入：
-      - draft_content: str，待评分的文稿内容
-      - audience_profile: str，受众画像描述（建议引用docs/受众描述-中等家长.md）
-      - scoring_criteria: str，评分标准（如内容相关性、建议可操作性、语言通俗性等）
+      - tool_context: ToolContext，用于访问状态和获取当前文稿
+      - audience_profile: Optional[str]，受众画像描述（如果未提供，则使用默认值）
+      - scoring_criteria: Optional[str]，评分标准（如果未提供，则从状态获取或使用默认值）
     输出：
-      - score: int，0-100分
-      - feedback: str，详细反馈
-      - key_issues: list[str]，关键问题列表，便于后续自动分析
+      - status: str, "llm_prompt_ready" 或 "error"
+      - prompt: Optional[str], 生成的LLM提示词 (如果成功)
+      - message: Optional[str], 错误信息 (如果失败)
+      - audience_type: Optional[str], "parents" (如果成功)
     """
-    # 日志记录
-    logger.info(f"开始对文稿进行评分，文稿长度: {len(draft_content)}")
-    logger.warning("[SCORING_DEBUG] 开始对文稿进行评分...")
+    logger.info("开始生成家长评分提示词...")
+    logger.warning("[SCORING_DEBUG] 开始生成家长评分提示词...")
+    
+    # 使用状态管理器
+    state_manager = StateManager(tool_context)
     
-    # 保留特殊情况的快速返回逻辑
-    if not draft_content.strip():
-        logger.warning("文稿内容为空，直接返回低分结果")
+    # 从状态获取当前文稿
+    draft_content = state_manager.get(CURRENT_DRAFT_KEY)
+    if not draft_content:
+        logger.error("无法评分：在会话状态中找不到当前文稿 (CURRENT_DRAFT_KEY)。")
         return {
-            "score": 0,
-            "feedback": "内容为空，请补充文稿内容。",
-            "key_issues": ["内容为空"]
+            "status": "error",
+            "message": "无法评分，因为在状态中找不到当前文稿。"
         }
+    
+    logger.info(f"获取到待评分文稿，长度: {len(draft_content)}")
+
+    # 获取评分标准：优先使用传入参数，否则从状态获取，最后使用默认值
+    if not scoring_criteria:
+        scoring_criteria = state_manager.get(INITIAL_SCORING_CRITERIA_KEY, "内容相关性、建议可操作性、语言通俗性")
+        logger.info(f"从状态或默认值获取评分标准: {scoring_criteria}")
+    
+    # 获取受众画像：优先使用传入参数，否则使用默认值
+    if not audience_profile:
+        audience_profile = "中等家长，孩子小学高年级至高一，关注内容实用性和语言通俗易懂" # 默认画像
+        logger.info(f"使用默认受众画像: {audience_profile}")
         
     # 构建提示词
     prompt = PARENTS_SCORING_PROMPT_TEMPLATE.format(
-        audience_profile=audience_profile,
-        scoring_criteria=scoring_criteria,
-        draft_content=draft_content
+        audience_profile=audience_profile, # 使用获取到的值
+        scoring_criteria=scoring_criteria, # 使用获取到的值
+        draft_content=draft_content # 使用从状态获取的值
     )
     
-    # 如果提供了工具上下文，记录评分事件
-    if tool_context:
-        log_generation_event("scoring_prompt_generated", {
-            "prompt_preview": prompt[:100] + "...",
-            "draft_length": len(draft_content)
-        }, {
-            "audience_type": "parents"
-        })
+    # 记录评分事件（tool_context 已保证存在）
+    log_generation_event("scoring_prompt_generated", {
+        "prompt_preview": prompt[:100] + "...",
+        "draft_length": len(draft_content)
+    }, {
+        "audience_type": "parents"
+    })
+    
     print("[SCORING_DEBUG] score_for_parents prompt:", prompt[:200])
     # 返回提示词，供Agent处理
     return {
@@ -114,20 +132,28 @@ def parse_scoring_result(llm_output: str) -> Dict[str, Any]:
     }
     
     try:
-        # 优先匹配"1. 数字"格式
-        score_match = re.search(r'1\.\s*([1-9][0-9]?|100)\b', llm_output)
+        # 优先匹配"分数：85"或"分数: 85"格式
+        score_match = re.search(r'分数[:：]\s*([1-9][0-9]?|100)\b', llm_output)
         if score_match:
             score = int(score_match.group(1))
             if 0 <= score <= 100:
                 result["score"] = score
-                logger.info(f"解析到分数: {score}")
+                logger.info(f"解析到分数(分数:): {score}")
         else:
-            # 兜底：匹配第一个0-100的独立整数
-            score_match = re.search(r'\b([1-9][0-9]?|100)\b', llm_output)
+            # 优先匹配"1. 数字"格式
+            score_match = re.search(r'1\.\s*([1-9][0-9]?|100)\b', llm_output)
             if score_match:
                 score = int(score_match.group(1))
-                result["score"] = score
-                logger.info(f"兜底解析到分数: {score}")
+                if 0 <= score <= 100:
+                    result["score"] = score
+                    logger.info(f"解析到分数: {score}")
+            else:
+                # 兜底：匹配第一个0-100的独立整数
+                score_match = re.search(r'\b([1-9][0-9]?|100)\b', llm_output)
+                if score_match:
+                    score = int(score_match.group(1))
+                    result["score"] = score
+                    logger.info(f"兜底解析到分数: {score}")
         
         # 提取关键问题 (尝试匹配"-"开头的列表项)
         key_issues = []
diff --git a/agents/draft_craft/draft_craft/tools/writing_tools.py b/agents/draft_craft/draft_craft/tools/writing_tools.py
index fe42eb9..1907966 100644
--- a/agents/draft_craft/draft_craft/tools/writing_tools.py
+++ b/agents/draft_craft/draft_craft/tools/writing_tools.py
@@ -25,7 +25,7 @@ from .state_manager import (
     INITIAL_SCORING_CRITERIA_KEY, CURRENT_DRAFT_KEY, CURRENT_SCORE_KEY,
     CURRENT_FEEDBACK_KEY, SCORE_THRESHOLD_KEY, ITERATION_COUNT_KEY, IS_COMPLETE_KEY
 )
-from .logging_utils import log_generation_event
+from .logging_utils import log_generation_event, log_llm_generation
 from .llm_generator import get_content_generator
 from .fix_llm import safely_run_async  # 重新添加，以便在需要时处理异步调用
 
@@ -126,20 +126,16 @@ def _generate_with_llm(
             
             logger.info(f"使用LLM撰写初始文稿。素材长度:{len(material)}，要求长度:{len(requirements)}，评分标准长度:{len(criteria)}")
             
-            # 创建提示词
+            # 创建更强提示词：包含结构、案例、字数和语言要求，并动态标注迭代轮数
             prompt = f"""
-你是一位专业文案写手。请基于以下素材和要求，撰写一篇高质量文章：
+你是一位资深文案写手。请在第{iteration_count+1}轮生成完整文章，严格遵循以下要求：
+- 结构：包含标题、引言、至少3个逻辑分节和结论，使用过渡词保持连贯。
+- 案例：务必插入至少5个真实案例，每个案例包含背景、过程和结果，并标明来源。
+- 语言：精准、优美、有说服力，句式多样，避免重复。
+- 字数：符合写作要求中指定的篇幅（{requirements}）。
+- 评分标准：{criteria}
 
-## 素材
-{material}
-
-## 写作要求
-{requirements}
-
-## 评分标准
-{criteria}
-
-请直接输出文章内容，不要添加额外说明。
+请直接输出完整文章正文，不要附加调试信息、编号列表或额外说明。
 """
             
             # 生成备用内容
@@ -176,6 +172,7 @@ def _generate_with_llm(
                     if result and not result.startswith("生成文稿失败") and not "error" in result.lower():
                         draft = result
                         logger.info(f"LLM生成成功，得到{len(draft)}字符的内容")
+                        log_llm_generation(iteration_count, prompt, draft)
                     else:
                         logger.warning("LLM生成失败或返回错误信息，使用备用内容")
                         draft = backup_draft
@@ -204,18 +201,16 @@ def _generate_with_llm(
             
             # 创建改进提示词
             prompt = f"""
-你是一位专业文案写手。请基于以下反馈，改进现有文稿：
-
-## 当前文稿
-{current_draft}
-
-## 评分反馈
-{feedback if feedback else "没有具体反馈，请对文稿进行一般性改进，使其更全面、更有深度。"}
+你是一位资深文案写手。请在第{iteration_count+1}轮基于以下反馈**直接输出完整改进后文章**，并确保满足：
+- 案例：插入至少5个真实案例，每个包含背景、过程和结果。
+- 内容：深化细节和分析，满足写作要求（{requirements}）。
+- 结构：保留原有逻辑，段落清晰，有过渡词。
+- 语言：精准优美、句式多样、易于阅读。
+- 评分标准：{criteria}
 
-## 评分标准
-{criteria}
+反馈如下：{feedback if feedback else "无反馈，请全面改进内容"}
 
-请输出完整的改进后文稿，不要添加额外说明。
+请仅输出完整文章正文，禁止仅输出建议或要点。
 """
             
             # 生成备用改进内容
@@ -244,6 +239,7 @@ def _generate_with_llm(
                     if result and not (result.startswith(current_draft + "\n\n[文稿同步改进失败") or "error" in result.lower()):
                         draft = result
                         logger.info(f"LLM改进成功，得到{len(draft)}字符的内容")
+                        log_llm_generation(iteration_count, prompt, draft)
                     else:
                         logger.warning("LLM改进失败或返回错误信息，使用备用内容")
                         draft = current_draft + backup_improvement
